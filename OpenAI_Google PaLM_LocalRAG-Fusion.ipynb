{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q google-generativeai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import app packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    m for m in palm.list_models() if \"generateText\" in m.supported_generation_methods\n",
    "]\n",
    "\n",
    "for m in models:\n",
    "    print(f\"Model Name: {m.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings and Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypdf\n",
    "%pip install faiss-cpu\n",
    "%pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langchain_experimental\n",
    "import google.generativeai\n",
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import google_palm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.document_loaders import (PyPDFLoader, DataFrameLoader)\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split a single pdf into pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pdf document using pypdfloader\n",
    "loader = PyPDFLoader(\"./data/City of Costa Mesa RFP Final- Posted.pdf\")\n",
    "# split each pdf page into a separate document and load into memory\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the numbe of pages split by the splitter\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a specific page\n",
    "pages[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Local Vectordb Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Embeddings source\n",
    "embeddings=GooglePalmEmbeddings(model_name=\"models/embedding-gecko-001\",google_api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local vector db if doesn't already exist\n",
    "def embed_index(doc_list, embed_fn, index_store):\n",
    "  \"\"\"Function takes in existing vector_store, \n",
    "  new doc_list and embedding function that is \n",
    "  initialized on appropriate model. Local or online. \n",
    "  New embedding is merged with the existing index. If no \n",
    "  index given a new one is created\"\"\"\n",
    "  #check whether the doc_list is documents, or text\n",
    "  try:\n",
    "    faiss_db = FAISS.from_documents(doc_list, \n",
    "                              embed_fn)  \n",
    "  except Exception as e:\n",
    "    faiss_db = FAISS.from_texts(doc_list, \n",
    "                              embed_fn)\n",
    "  \n",
    "  if os.path.exists(index_store):\n",
    "    local_db = FAISS.load_local(index_store,embed_fn)\n",
    "    #merging the new embedding with the existing index store\n",
    "    local_db.merge_from(faiss_db)\n",
    "    print(\"Merge completed\")\n",
    "    local_db.save_local(index_store)\n",
    "    print(\"Updated index saved\")\n",
    "  else:\n",
    "    faiss_db.save_local(folder_path=index_store)\n",
    "    print(\"New store created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test local vector db\n",
    "def get_docs_length(index_path, embed_fn):\n",
    "  test_index = FAISS.load_local(index_path,\n",
    "                              embeddings=embed_fn)\n",
    "  test_dict = test_index.docstore._dict\n",
    "  return len(test_dict.values())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Document Chunk Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_splits(pdf_file):\n",
    "  \"\"\"Function takes in the pdf data and returns the  \n",
    "  splits so for further processing can be done.\"\"\"\n",
    "  \n",
    "  loader = PyPDFLoader(pdf_file)\n",
    "  pages = loader.load_and_split()  \n",
    "\n",
    "  textSplit = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                             chunk_overlap=200,\n",
    "                                             length_function=len)\n",
    "  doc_list = []\n",
    "  #Pages will be list of pages, so need to modify the loop\n",
    "  for pg in pages:\n",
    "    pg_splits = textSplit.split_text(pg.page_content)\n",
    "    doc_list.extend(pg_splits)\n",
    "\n",
    "  return doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load split document chunks into local Vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs = get_pdf_splits(\"./data/City of Costa Mesa RFP Final- Posted.pdf\")\n",
    "\n",
    "embed_index(doc_list=pdf_docs,\n",
    "            embed_fn=embeddings,\n",
    "            index_store='./vectorstore/CMRFP_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdf_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check chunk split page content and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Local Vectordb size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_docs_length(index_path=\"./vectorstore/CMRFP_index\",\n",
    "                embed_fn=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test in-memory FAISS db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = FAISS.from_documents(pages, GooglePalmEmbeddings(model_name=\"models/embedding-gecko-001\",google_api_key=os.getenv(\"GOOGLE_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = faiss_index.similarity_search(\"what are the duties of the program manager\", k=5)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define FAISS db Local Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_localindex = FAISS.load_local(\"./vectorstore/CMRFP_index\",embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Local FAISS db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = faiss_localindex.similarity_search_with_relevance_scores(\"what does phase 2 deployment entail?\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = faiss_localindex.similarity_search(\"what does phase 2 deployment entail?\", k=5)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langsmith langchainhub\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain_experimental\n",
    "import google.generativeai as palm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "from langchain.llms import openai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import tracing_v2_enabled\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "LANGCHAIN_TRACING_V2=\"true\"\n",
    "LANGCHAIN_API_KEY=\"ls__c373b2a75ddb472dabcb4e8b2818ad36\"\n",
    "LANGCHAIN_PROJECT=\"palm-ragfusion-shadow-99\"  # if not specified, defaults to \"default\"\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_localindex.as_retriever(search_type=\"mmr\", search_kwargs={'k': 10, 'lambda_mult': 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare RAG Fusion Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://github.com/shivanshkaushikk/rag-fusion/blob/main/RAG-fusion.ipynb\n",
    "\n",
    "def generate_queries_chatgpt(original_query):\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Your task is to always think step by step to generate multiple different document search queries that aim to answer the user question from multiple perspectives. Each query MUST tackle the question from a different viewpoint, dissect the sentence clauses to infer question intent, we want to get a variety of RELEVANT search results. Each query MUST be in one line and one line only. You SHOULD NOT include any preamble or explanations, and you SHOULD NOT answer the questions or add anything else, just geenrate the queries.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate multiple search queries related to: {original_query}\"},\n",
    "            {\"role\": \"user\", \"content\": \"OUTPUT (5 queries):\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    generated_queries = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "    return generated_queries\n",
    "\n",
    "def vector_search(query):\n",
    "    search_results = {}\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    for i in retrieved_docs:\n",
    "        search_results[i.page_content] = i.metadata=1\n",
    "    return search_results\n",
    "\n",
    "def reciprocal_rank_fusion(search_results_dict, k=60):\n",
    "    fused_scores = {}\n",
    "        \n",
    "    for query, doc_scores in search_results_dict.items():\n",
    "        \n",
    "        for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n",
    "            if doc not in fused_scores:\n",
    "                fused_scores[doc] = 0\n",
    "            previous_score = fused_scores[doc]\n",
    "            fused_scores[doc] += 1 / (rank + k)\n",
    "            print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n",
    "\n",
    "    reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "    print(\"Final reranked results:\", reranked_results)\n",
    "    return reranked_results\n",
    "\n",
    "def generate_output(original_query, reranked_results):\n",
    "    reranked_docs = [i for i in reranked_results.keys()]\n",
    "    context = '\\n'.join(reranked_docs)\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers user's questions based on the context provided. You always format answers in paragraphs, outlines, or tables when appropriate.\\nDo not make up an answer if you do not know it, stay within the bounds of the context provided, if you don't know the answer, say that you don't have enough information on the topic!\"},\n",
    "            {\"role\": \"user\", \"content\": f\"CONTEXT: {context}\\nQUERY: {original_query}\"},\n",
    "            {\"role\": \"user\", \"content\": \"ANSWER:\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = response.choices[0].message.content.strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query = \"why does the City of costa mesa need a Enterprise Resource Planning System, are they having any issues or pain points with their current syste? Do they currently have an Enterprise Resource Planning System?\"\n",
    "generated_queries = generate_queries_chatgpt(original_query)\n",
    "\n",
    "all_results = {}\n",
    "for query in generated_queries:\n",
    "    search_results = vector_search(query)\n",
    "    all_results[query] = search_results\n",
    "\n",
    "reranked_result = reciprocal_rank_fusion(all_results)\n",
    "final_output = generate_output(original_query, reranked_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART IV: \n",
    "\n",
    "CSV Custom Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.agents import AgentExecutor, OpenAIFunctionsAgent\n",
    "from langchain.agents.agent_toolkits.conversational_retrieval.tool import (\n",
    "    create_retriever_tool,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "from langchain.tools import PythonAstREPLTool\n",
    "from langchain.vectorstores import FAISS\n",
    "from langsmith import Client\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Embeddings source\n",
    "embeddings=GooglePalmEmbeddings(model_name=\"models/embedding-gecko-001\",google_api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envChainlit312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
